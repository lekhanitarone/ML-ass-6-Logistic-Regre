{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1: Logistic Regression vs Linear Regression**\n",
        ">Logistic Regression is a supervised learning algorithm used mainly for classification problems, especially binary outcomes like spam/not spam or disease/no disease. It models the relationship between independent variables and a categorical dependent variable by estimating probabilities. Unlike Linear Regression, which predicts continuous numeric outputs by fitting a straight line, Logistic Regression predicts the probability of belonging to a class. To achieve this, it applies the logistic (sigmoid) function to the linear combination of input features, ensuring outputs always lie between 0 and 1. Linear Regression can give values outside this range, making it unsuitable for classification. Logistic Regression also uses a decision boundary (usually 0.5) to assign the final class label.\n",
        "\n",
        "\n",
        "\n",
        "**Question 2: Role of Sigmoid Function in Logistic Regression**\n",
        ">The Sigmoid function plays a crucial role in Logistic Regression by converting the linear equation’s output into a probability value between 0 and 1. Its S-shaped curve allows the model to represent uncertainty and smooth transitions between classes. When the input to the sigmoid is very large positive, the output approaches 1; when very negative, it approaches 0. This makes it ideal for mapping inputs to binary class probabilities. The function ensures that Logistic Regression outputs remain interpretable as probabilities rather than raw scores. Additionally, because the sigmoid is differentiable, it supports optimization using gradient descent, which helps in finding the best model parameters. Without it, Logistic Regression would lose its classification capability and behave like Linear Regression.\n",
        "\n",
        "\n",
        "\n",
        "**Question 3: What is Regularization in Logistic Regression and why is it needed?**\n",
        ">Regularization in Logistic Regression is a technique used to prevent the model from overfitting the training data. Overfitting happens when the model learns noise or irrelevant patterns, which reduces its performance on unseen data. Regularization works by adding a penalty term to the cost function, discouraging the model from assigning very high weights to features. Two common types are L1 (Lasso) and L2 (Ridge) regularization. L1 helps in feature selection by shrinking some coefficients to zero, while L2 reduces the magnitude of coefficients but keeps them all. By controlling complexity, regularization improves generalization and ensures the model performs better on new datasets.\n",
        "\n",
        "\n",
        "\n",
        "**Question 4: What are some common evaluation metrics for classification models, and why are they important?**\n",
        ">Common evaluation metrics for classification models include accuracy, precision, recall, F1-score, and ROC-AUC. Accuracy measures the overall proportion of correct predictions, but it may be misleading with imbalanced data. Precision tells how many predicted positives are actually correct, while recall shows how many actual positives are correctly identified. The F1-score is the harmonic mean of precision and recall, balancing both aspects. ROC-AUC evaluates how well the model separates classes across different thresholds. These metrics are important because they provide deeper insights into model performance beyond simple accuracy, helping to choose the right model for real-world applications where class distribution may vary.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ixP2J2Z0UFI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "# splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "# (Use Dataset from sklearn package)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train/Test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)  # increased iterations for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of Logistic Regression model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2cfaU5OUz7h",
        "outputId": "c79e1aef-fe0b-46d7-af49-38ed4b4a9231"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression model: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program to train a Logistic Regression model using L2\n",
        "# regularization (Ridge) and print the model coefficients and accuracy.\n",
        "# (Use Dataset from sklearn package)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression with L2 Regularization (default penalty='l2')\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print coefficients and accuracy\n",
        "print(\"Model Coefficients:\\n\", model.coef_)\n",
        "print(\"\\nModel Intercept:\\n\", model.intercept_)\n",
        "print(\"\\nAccuracy of Logistic Regression with L2 regularization:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcE49OMwVAUg",
        "outputId": "a038574a-d043-4c0f-a84d-12333c427f13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            " [[ 1.0274368   0.22145051 -0.36213488  0.0254667  -0.15623532 -0.23771256\n",
            "  -0.53255786 -0.28369224 -0.22668189 -0.03649446 -0.09710208  1.3705667\n",
            "  -0.18140942 -0.08719575 -0.02245523  0.04736092 -0.04294784 -0.03240188\n",
            "  -0.03473732  0.01160522  0.11165329 -0.50887722 -0.01555395 -0.016857\n",
            "  -0.30773117 -0.77270908 -1.42859535 -0.51092923 -0.74689363 -0.10094404]]\n",
            "\n",
            "Model Intercept:\n",
            " [28.64871395]\n",
            "\n",
            "Accuracy of Logistic Regression with L2 regularization: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to train a Logistic Regression model for multiclass\n",
        "# classification using multi_class='ovr' and print the classification report.\n",
        "# (Use Dataset from sklearn package)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Iris dataset - multiclass)\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression for multiclass (One-vs-Rest)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for Logistic Regression (OvR):\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjhMNn7VLUs",
        "outputId": "12f8578a-bb0d-4064-bbff-fb2a10c300da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Logistic Regression (OvR):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "# hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "# accuracy.\n",
        "# (Use Dataset from sklearn package)\n",
        "\n",
        "# Question 8: Hyperparameter tuning with GridSearchCV for Logistic Regression\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Breast Cancer dataset)\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=10000, solver='liblinear')  # solver compatible with l1 and l2\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],        # Regularization strength\n",
        "    'penalty': ['l1', 'l2']              # Regularization type\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = grid.best_estimator_.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy with Best Parameters:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eUaX7L1VbxV",
        "outputId": "d742780d-72aa-4493-b177-d3f6da680daa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.9670329670329672\n",
            "Test Accuracy with Best Parameters: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to standardize the features before training Logistic\n",
        "# Regression and compare the model's accuracy with and without scaling.\n",
        "# (Use Dataset from sklearn package)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=10000, solver='lbfgs')\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "model_scaling = LogisticRegression(max_iter=10000, solver='lbfgs')\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy without Scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", accuracy_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8NHqq8HVnji",
        "outputId": "3c6b2547-c332-4096-a3c2-35f8c98ae0b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.956140350877193\n",
            "Accuracy with Scaling: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        ">To build a Logistic Regression model for predicting customer response in an imbalanced dataset (5% response rate), the first step would be data preprocessing by handling missing values, encoding categorical variables, and standardizing numerical features for consistent scaling. Since the dataset is highly imbalanced, techniques like SMOTE (Synthetic Minority Oversampling Technique), undersampling, or class weighting (class_weight='balanced' in Logistic Regression) should be applied to handle the imbalance. After that, feature selection can be done to remove irrelevant predictors, improving model efficiency and interpretability. Next, apply Logistic Regression with scaling (using StandardScaler) to avoid bias due to different feature magnitudes. For hyperparameter tuning, use GridSearchCV or RandomizedSearchCV to optimize C (regularization strength), penalty type (L1/L2), and solver. For evaluation, rely not only on accuracy (which is misleading in imbalanced data) but also on precision, recall, F1-score, ROC-AUC, and Precision-Recall curves to measure business effectiveness. Finally, the chosen model should be validated on unseen test data and monitored continuously, since in business cases, recall (catching most responders) may be more valuable than accuracy."
      ],
      "metadata": {
        "id": "r8sIT3j8V5uV"
      }
    }
  ]
}